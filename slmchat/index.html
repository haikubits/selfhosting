<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <title>In-Browser SLM Chat (WebGPU + WebLLM)</title>
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <style>
    :root { color-scheme: light dark; }
    body { font-family: system-ui, -apple-system, Segoe UI, Roboto, Helvetica, Arial, sans-serif; margin: 0; }
    header { padding: 12px 16px; border-bottom: 1px solid #ddd; display: flex; flex-wrap: wrap; gap: 8px; align-items: center; }
    main { max-width: 900px; margin: 0 auto; padding: 16px; }
    #chat { border: 1px solid #ddd; border-radius: 12px; padding: 12px; height: 60vh; overflow: auto; background: rgba(0,0,0,0.02); }
    .msg { padding: 10px 12px; border-radius: 10px; margin: 8px 0; max-width: 78%; line-height: 1.35; white-space: pre-wrap; }
    .user { background: #e8f0fe; align-self: flex-end; margin-left: auto; }
    .bot { background: #f1f3f4; }
    .sys { font-size: 12px; opacity: .7; }
    form { display: grid; grid-template-columns: 1fr auto; gap: 8px; margin-top: 12px; }
    textarea { resize: vertical; min-height: 48px; padding: 10px; border-radius: 10px; border: 1px solid #ccc; }
    button { padding: 10px 14px; border-radius: 10px; border: 0; background: #1a73e8; color: #fff; font-weight: 600; cursor: pointer; }
    button:disabled { opacity: .6; cursor: not-allowed; }
    .row { display: flex; gap: 8px; align-items: center; flex-wrap: wrap; }
    select, input[type="checkbox"] { padding: 8px; border-radius: 8px; border: 1px solid #ccc; }
    #status { font-size: 12px; opacity: .8; }
    .tiny { font-size: 12px; opacity: .8; }
    .spacer { flex: 1; }
    .hidden { display: none; }
  </style>
</head>
<body>
  <header>
    <div class="row" style="gap:12px;">
      <strong>Browser SLM Chat</strong>
      <span id="status">checking WebGPU…</span>
    </div>
    <div class="spacer"></div>
    <div class="row">
      <label>
        Model:
        <select id="model">
          <!-- These are pre-quantized, edge-friendly variants hosted by MLC/WebLLM -->
          <option value="Llama-3.2-1B-Instruct-q4f16_1-MLC" selected>Llama 3.2 1B Instruct (q4f16)</option>
          <option value="Phi-3-mini-4k-instruct-q4f16_1-MLC">Phi-3 Mini 4K Instruct (q4f16)</option>
          <option value="Qwen2.5-1.5B-Instruct-q4f16_1-MLC">Qwen 2.5 1.5B Instruct (q4f16)</option>
          <option value="Gemma-2-2B-it-q4f16_1-MLC">Gemma 2 2B IT (q4f16)</option>
        </select>
      </label>
      <label title="Keeps weights & KV cache in browser storage when possible">
        <input type="checkbox" id="persist" checked />
        persist
      </label>
      <button id="load">Load model</button>
      <button id="reset" disabled>Reset</button>
    </div>
  </header>

  <main>
    <div id="chat" aria-live="polite"></div>

    <form id="composer">
      <textarea id="input" placeholder="Ask me anything… (runs fully in your browser)"></textarea>
      <button id="send" disabled>Send</button>
    </form>

    <p class="tiny">
      Tip: First run downloads the model shards (size depends on model). Subsequent loads are fast thanks to caching. Requires a recent Chromium browser with WebGPU.
    </p>
  </main>

  <script type="module">
    // Import the WebLLM engine via ESM. Uses a dedicated Web Worker so the UI stays responsive.
    import { CreateWebWorkerMLCEngine } from "https://esm.sh/@mlc-ai/web-llm@0.2.78";

    const $ = (sel) => document.querySelector(sel);
    const chatEl = $("#chat");
    const inputEl = $("#input");
    const sendBtn = $("#send");
    const loadBtn = $("#load");
    const resetBtn = $("#reset");
    const statusEl = $("#status");
    const modelSel = $("#model");
    const persistEl = $("#persist");

    /** Chat state (OpenAI-style) */
    let messages = [
      { role: "system", content: "You are a helpful, concise AI that answers clearly." }
    ];

    /** WebLLM engine handle */
    let engine = null;
    let loading = false;
    let streaming = false;

    // Basic WebGPU check
    if (!("gpu" in navigator)) {
      status("⚠️ WebGPU not available. Try Chrome/Edge 121+ (chrome://flags → enable WebGPU on some platforms).");
    } else {
      status("WebGPU detected. Pick a model and click Load.");
    }

    // UI helpers
    function status(text) { statusEl.textContent = text; }
    function addMsg(role, text, extraClass="") {
      const div = document.createElement("div");
      div.className = `msg ${role} ${extraClass}`;
      div.textContent = text;
      chatEl.appendChild(div);
      chatEl.scrollTop = chatEl.scrollHeight;
      return div;
    }
    function setBusy(b) {
      sendBtn.disabled = b || !engine;
      inputEl.disabled = b || !engine;
      loadBtn.disabled = b;
      resetBtn.disabled = b || !engine;
    }

    // Initialize / load model
    loadBtn.addEventListener("click", async () => {
      if (loading) return;
      loading = true;
      setBusy(true);
      chatEl.innerHTML = "";
      addMsg("sys", "Initializing engine & downloading model (first run only)…", "sys");

      try {
        // Create a WebWorker engine so downloads & compute don't block the UI thread
        const worker = new Worker("https://esm.sh/@mlc-ai/web-llm@0.2.78/engine/worker?module", { type: "module" });

        // Model id from selector
        const modelId = modelSel.value;

        // Optional persistent caching (IndexedDB)
        const appConfig = {
          model_list: [{ model_url: modelId }],
          use_web_worker: true,
          cache_system_prompt: true,
          kv_cache_config: { max_num_pages: 32 }
        };

        const initConfig = {
          // Persist weights to storage when possible to avoid re-downloading
          appConfig,
          initProgressCallback: (report) => {
            // report: { progress: 0..1, text: "..." }
            status(`Loading: ${(report.progress * 100).toFixed(1)}% — ${report.text}`);
          },
          // Store engine & weights when chosen
          // (WebLLM handles caching automatically; this flag is only indicative in UI)
        };

        engine = await CreateWebWorkerMLCEngine(worker, { model: modelId }, initConfig);

        addMsg("sys", `✅ Loaded ${modelId}. You can chat now.`, "sys");
        status("Ready.");
        setBusy(false);
      } catch (err) {
        console.error(err);
        status("Error loading model. See console.");
        addMsg("sys", `❌ ${String(err)}`, "sys");
        engine = null;
        setBusy(false);
      } finally {
        loading = false;
      }
    });

    // Reset conversation (keeps the model)
    resetBtn.addEventListener("click", () => {
      messages = [
        { role: "system", content: "You are a helpful, concise AI that answers clearly." }
      ];
      chatEl.innerHTML = "";
      addMsg("sys", "Conversation reset.", "sys");
    });

    // Send a chat message
    $("#composer").addEventListener("submit", async (e) => {
      e.preventDefault();
      if (!engine || streaming) return;

      const userText = inputEl.value.trim();
      if (!userText) return;

      inputEl.value = "";
      const u = addMsg("user", userText);
      const a = addMsg("bot", ""); // we'll stream into this node

      messages.push({ role: "user", content: userText });

      try {
        streaming = true;
        setBusy(true);
        status("Thinking…");

        // Stream tokens with OpenAI-compatible API
        const stream = await engine.chat.completions.create({
          messages,
          stream: true,
          // You can tweak max_tokens/temperature/top_p here
          max_tokens: 512,
          temperature: 0.7,
        });

        let acc = "";
        for await (const chunk of stream) {
          // Each chunk contains delta tokens
          const delta = chunk?.choices?.[0]?.delta?.content ?? "";
          if (delta) {
            acc += delta;
            a.textContent = acc;
            chatEl.scrollTop = chatEl.scrollHeight;
          }
        }

        // Add assistant message to history
        messages.push({ role: "assistant", content: acc || a.textContent });
        status("Ready.");
      } catch (err) {
        console.error(err);
        status("Error while generating. See console.");
        a.textContent = `❌ ${String(err)}`;
      } finally {
        streaming = false;
        setBusy(false);
      }
    });

    // Enable send after model is loaded
    const observer = new MutationObserver(() => {
      sendBtn.disabled = !engine || streaming;
    });
    observer.observe(statusEl, { childList: true });

    // Persist toggle is informational; WebLLM caches by default.
    persistEl.addEventListener("change", () => {
      // no-op: included to show intent; WebLLM manages cache automatically.
    });
  </script>
</body>
</html>
