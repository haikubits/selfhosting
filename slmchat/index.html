<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <title>Browser SLM Chat — WebGPU or WASM (Single File)</title>
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <style>
    :root { color-scheme: light dark; }
    body { margin:0; font-family: system-ui, -apple-system, Segoe UI, Roboto, Helvetica, Arial, sans-serif; }
    header { padding: 12px 16px; border-bottom: 1px solid #ddd; display:flex; flex-wrap:wrap; gap:8px; align-items:center; }
    main { max-width: 900px; margin: 0 auto; padding: 16px; }
    #chat { border: 1px solid #ddd; border-radius: 12px; padding: 12px; height: 60vh; overflow: auto; background: rgba(0,0,0,0.02); }
    .msg { padding: 10px 12px; border-radius: 10px; margin: 8px 0; max-width: 78%; line-height: 1.35; white-space: pre-wrap; }
    .user { background: #e8f0fe; margin-left: auto; }
    .bot { background: #f1f3f4; }
    .sys { font-size: 12px; opacity: .75; }
    form { display: grid; grid-template-columns: 1fr auto; gap: 8px; margin-top: 12px; }
    textarea { resize: vertical; min-height: 52px; padding: 10px; border-radius: 10px; border: 1px solid #ccc; }
    button { padding: 10px 14px; border-radius: 10px; border: 0; background: #1a73e8; color: #fff; font-weight: 600; cursor: pointer; }
    button:disabled { opacity: .6; cursor: not-allowed; }
    .row { display: flex; gap: 8px; align-items: center; flex-wrap: wrap; }
    select { padding: 8px; border-radius: 8px; border: 1px solid #ccc; }
    #status { font-size: 12px; opacity: .8; }
    .hidden { display:none; }
  </style>
</head>
<body>
  <header>
    <div class="row" style="gap:12px;">
      <strong>Browser SLM Chat</strong>
      <span id="status">Detecting capabilities…</span>
    </div>
    <div style="margin-left:auto" class="row">
      <label id="gpu-model-wrap" class="hidden">
        GPU Model:
        <select id="gpu-model">
          <!-- WebLLM edge-friendly quantized models -->
          <option value="Llama-3.2-1B-Instruct-q4f16_1-MLC" selected>Llama 3.2 1B Instruct (q4f16)</option>
          <option value="Phi-3-mini-4k-instruct-q4f16_1-MLC">Phi-3 Mini 4K Instruct (q4f16)</option>
          <option value="Qwen2.5-1.5B-Instruct-q4f16_1-MLC">Qwen 2.5 1.5B Instruct (q4f16)</option>
          <option value="Gemma-2-2B-it-q4f16_1-MLC">Gemma 2 2B IT (q4f16)</option>
        </select>
      </label>
      <label id="wasm-model-wrap" class="hidden">
        CPU Model:
        <select id="wasm-model">
          <!-- Transformers.js ONNX models (WASM/CPU) -->
          <option value="Xenova/TinyLlama-1.1B-Chat-v1.0" selected>TinyLlama-1.1B-Chat</option>
          <option value="Xenova/distilgpt2">DistilGPT-2 (tiny)</option>
          <option value="Xenova/flan-t5-small">FLAN-T5-small (instr.)</option>
        </select>
      </label>
      <button id="load">Load</button>
      <button id="reset" disabled>Reset</button>
    </div>
  </header>

  <main>
    <div id="chat" aria-live="polite"></div>
    <form id="composer">
      <textarea id="input" placeholder="Ask me anything… runs fully in your browser"></textarea>
      <button id="send" disabled>Send</button>
    </form>
    <p class="sys" id="hint">First run downloads model files (cached for subsequent loads).</p>
  </main>

  <script type="module">
    // Imports (lazy where possible to reduce first paint)
    let CreateWebWorkerMLCEngine; // from @mlc-ai/web-llm
    let pipeline, env;            // from @huggingface/transformers

    const $ = s => document.querySelector(s);
    const chatEl   = $("#chat");
    const inputEl  = $("#input");
    const sendBtn  = $("#send");
    const loadBtn  = $("#load");
    const resetBtn = $("#reset");
    const statusEl = $("#status");

    const gpuModelSel  = $("#gpu-model");
    const wasmModelSel = $("#wasm-model");
    const gpuModelWrap  = $("#gpu-model-wrap");
    const wasmModelWrap = $("#wasm-model-wrap");

    // Chat state
    let messages = [{ role: "system", content: "You are a helpful, concise assistant." }];
    let mode = "idle"; // "gpu" | "wasm"
    let engine = null; // WebLLM engine
    let generator = null; // Transformers.js pipeline
    let streaming = false;

    function addMsg(role, text, extra="") {
      const div = document.createElement("div");
      div.className = `msg ${role} ${extra}`;
      div.textContent = text;
      chatEl.appendChild(div);
      chatEl.scrollTop = chatEl.scrollHeight;
      return div;
    }
    function status(t) { statusEl.textContent = t; }
    function setBusy(b) {
      sendBtn.disabled = b || (mode==="gpu" ? !engine : !generator);
      inputEl.disabled = b || (mode==="gpu" ? !engine : !generator);
      loadBtn.disabled = b;
      resetBtn.disabled = b || (mode==="gpu" ? !engine : !generator);
    }

    // Capability detect
    const hasWebGPU = !!navigator.gpu;
    if (hasWebGPU) {
      mode = "gpu";
      status("WebGPU detected — GPU mode ready.");
      gpuModelWrap.classList.remove("hidden");
    } else {
      mode = "wasm";
      status("No WebGPU — falling back to WASM/CPU mode.");
      wasmModelWrap.classList.remove("hidden");
    }

    // ---------- GPU PATH (WebLLM with blob: Worker) ----------
    async function initGPU(modelId) {
      setBusy(true);
      status("Initializing engine & downloading model (first run only)…");
      try {
        // Load WebLLM entry
        ({ CreateWebWorkerMLCEngine } = await import("https://esm.sh/@mlc-ai/web-llm@0.2.78"));

        // Create a same-origin module worker via blob to avoid cross-origin restrictions on GH Pages
        const workerCode = `
          import { WebWorkerMLCEngineHandler } from "https://esm.sh/@mlc-ai/web-llm@0.2.78";
          const h = new WebWorkerMLCEngineHandler();
          self.onmessage = (e) => h.onmessage(e);
        `;
        const blob = new Blob([workerCode], { type: "text/javascript" });
        const workerUrl = URL.createObjectURL(blob);
        const worker = new Worker(workerUrl, { type: "module" });

        engine = await CreateWebWorkerMLCEngine(
          worker,
          { model: modelId },
          {
            initProgressCallback: (r) => {
              if (r && typeof r.progress === "number") {
                status(`Loading: ${(r.progress*100).toFixed(1)}% — ${r.text || ""}`);
              } else {
                status("Preparing…");
              }
            },
          }
        );

        addMsg("sys", `✅ GPU model loaded: ${modelId}`, "sys");
        status("Ready.");
      } catch (e) {
        console.error(e);
        addMsg("sys", `❌ GPU init failed. Falling back to WASM. (${e})`, "sys");
        mode = "wasm";
        gpuModelWrap.classList.add("hidden");
        wasmModelWrap.classList.remove("hidden");
        await initWASM(wasmModelSel.value); // auto-fallback
      } finally {
        setBusy(false);
        sendBtn.disabled = !(mode==="gpu" ? !!engine : !!generator);
      }
    }

    async function sendGPU(userText, node) {
      streaming = true;
      setBusy(true);
      status("Thinking…");
      try {
        const stream = await engine.chat.completions.create({
          messages,
          stream: true,
          max_tokens: 512,
          temperature: 0.7,
        });
        let acc = "";
        for await (const chunk of stream) {
          const delta = chunk?.choices?.[0]?.delta?.content ?? "";
          if (delta) {
            acc += delta;
            node.textContent = acc;
            chatEl.scrollTop = chatEl.scrollHeight;
          }
        }
        messages.push({ role: "assistant", content: acc });
        status("Ready.");
      } catch (err) {
        console.error(err);
        node.textContent = `❌ ${String(err)}`;
        status("Error.");
      } finally {
        streaming = false;
        setBusy(false);
      }
    }

    // ---------- WASM PATH (Transformers.js) ----------
    async function initWASM(modelId) {
      setBusy(true);
      status("Downloading model (first time only)…");
      try {
        ({ pipeline, env } = await import("https://esm.sh/@huggingface/transformers@3.0.0"));
        // Keep things smooth on phones
        env.allowRemoteModels = true;
        env.useFS = false; // avoid OPFS prompts on iOS
        env.backends ??= {};
        env.backends.onnx ??= {};
        env.backends.onnx.wasm ??= {};
        env.backends.onnx.wasm.numThreads = 1;

        generator = await pipeline("text-generation", modelId, {
          dtype: "q8",
          device: "wasm", // ensures CPU/WASM path
          progress_callback: ({ progress, loaded, total }) => {
            if (total) status(`Downloading ${(100*progress).toFixed(1)}% (${(loaded/1e6).toFixed(1)} / ${(total/1e6).toFixed(1)} MB)…`);
          },
        });

        addMsg("sys", `✅ CPU model loaded: ${modelId}`, "sys");
        status("Ready.");
      } catch (e) {
        console.error(e);
        status("Failed to load CPU model. See console.");
        addMsg("sys", `❌ ${String(e)}`, "sys");
      } finally {
        setBusy(false);
        sendBtn.disabled = !generator;
      }
    }

    async function sendWASM(userText, node) {
      setBusy(true);
      status("Generating…");
      try {
        // Small rolling context for phones
        const lastTurns = messages.filter(m => m.role !== "system").slice(-3);
        const context = lastTurns.map(m => `${m.role === "user" ? "User" : "Assistant"}: ${m.content}`).join("\n");
        const prompt = `${context}\nUser: ${userText}\nAssistant:`;

        const out = await generator(prompt, {
          max_new_tokens: 128,
          do_sample: true,
          temperature: 0.7,
          top_p: 0.9,
          repetition_penalty: 1.1,
        });

        const full = out[0]?.generated_text || "";
        const reply = full.split("Assistant:").pop().trim() || full.trim();
        node.textContent = reply;
        messages.push({ role: "assistant", content: reply });
        status("Ready.");
      } catch (e) {
        console.error(e);
        node.textContent = "❌ Error generating response.";
        status("Error.");
      } finally {
        setBusy(false);
      }
    }

    // ---------- Wire up UI ----------
    loadBtn.addEventListener("click", async () => {
      chatEl.innerHTML = "";
      messages = [{ role: "system", content: "You are a helpful, concise assistant." }];
      mode === "gpu" ? await initGPU(gpuModelSel.value) : await initWASM(wasmModelSel.value);
      resetBtn.disabled = !(mode==="gpu" ? !!engine : !!generator);
      sendBtn.disabled = !(mode==="gpu" ? !!engine : !!generator);
    });

    resetBtn.addEventListener("click", () => {
      messages = [{ role: "system", content: "You are a helpful, concise assistant." }];
      chatEl.innerHTML = "";
      addMsg("sys", "Conversation reset.", "sys");
    });

    $("#composer").addEventListener("submit", async (e) => {
      e.preventDefault();
      if (streaming) return;

      const text = inputEl.value.trim();
      if (!text) return;
      inputEl.value = "";

      addMsg("user", text);
      const a = addMsg("bot", "");

      messages.push({ role: "user", content: text });

      if (mode === "gpu" && engine) {
        await sendGPU(text, a);
      } else if (generator) {
        await sendWASM(text, a);
      } else {
        a.textContent = "Model not loaded.";
      }
    });

    // Optional: auto-load a default model on first visit
    // mode === "gpu" ? initGPU(gpuModelSel.value) : initWASM(wasmModelSel.value);
  </script>
</body>
</html>
